{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0ce9d3308a83a37d22649dcd50c19e181fcfb7887b13f1c6f31fcc7446dbaf41b",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# R-CNN Implementation #\n",
    "### Modified code from https://www.pyimagesearch.com/2020/07/13/r-cnn-object-detection-with-keras-tensorflow-and-deep-learning/ by Adrian Rosebrock ###"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "source": [
    "# Getting dataset filepaths"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './clips'\n",
    "folder_path = os.listdir(dataset_path)\n",
    "imageset_path = [os.path.join(dataset_path, x) for x in folder_path]\n",
    "\n",
    "clean_dataset_path = './clean_dataset'\n",
    "positive_path = os.path.join(clean_dataset_path, \"CAR\")\n",
    "negative_path = os.path.join(clean_dataset_path, \"NO_CAR\")"
   ]
  },
  {
   "source": [
    "# Creating the training set\n",
    "### High computation time - Do not run. ###"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_PROPOSALS = 2000\n",
    "MAX_POSITIVE = 5\n",
    "MAX_NEGATIVE = 5\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "\n",
    "imageDataset = []\n",
    "labelSet = []\n",
    "for path in imageset_path:\n",
    "    annotation = os.path.join(path, 'annotation.json')\n",
    "    groundTruth = []\n",
    "    xStart, yStart, xEnd, yEnd = getCar(annotation) # top, bottom, left, right\n",
    "    groundTruth.extend((xStart, yStart, xEnd, yEnd)) # left, top, right, bottom\n",
    "    imagePath = os.path.join(path, 'imgs/040.jpg')\n",
    "    img_40 = cv2.imread(imagePath)\n",
    "\n",
    "    # Getting proposal boxes\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(img_40)\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "    rects = ss.process()\n",
    "    proposedRects = []\n",
    "    for (x, y, w, h) in rects:\n",
    "        proposedRects.append((x, y, x + w, y + h)) # left, top, right, bottom\n",
    "        \n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    \n",
    "    # Begin selective search\n",
    "    for proposedRect in proposedRects[:1000]:\n",
    "        (xStart, yStart, xEnd, yEnd) = proposedRect\n",
    "        iou = computeIoU(groundTruth, proposedRect)\n",
    "\n",
    "        roi = None\n",
    "\n",
    "        # if iou > 0.9 => positive else if iou < 0.05 => negative\n",
    "        if iou > 0.9 and positive < MAX_POSITIVE:\n",
    "            roi = img_40[yStart:yEnd, xStart:xEnd]\n",
    "            roi = cv2.resize(roi, (224, 224))\n",
    "            filename = \"{}.jpg\".format(positive_count)\n",
    "            outputPath = os.path.join(positive_path, filename)\n",
    "            cv2.imwrite(outputPath, roi)\n",
    "            positive_count += 1\n",
    "            positive += 1\n",
    "        \n",
    "        if iou < 0.05 and negative < MAX_NEGATIVE:\n",
    "            roi = img_40[yStart:yEnd, xStart:xEnd]\n",
    "            roi = cv2.resize(roi, (224, 224))\n",
    "            filename = \"{}.jpg\".format(negative_count)\n",
    "            outputPath = os.path.join(negative_path, filename)\n",
    "            cv2.imwrite(outputPath, roi)\n",
    "            negative_count += 1\n",
    "            negative += 1"
   ]
  },
  {
   "source": [
    "# Load dataset and create labels for positive and negative examples #"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "positiveFolder = os.listdir(positive_path)\n",
    "negativeFolder = os.listdir(negative_path)\n",
    "positivePath = [os.path.join(positive_path, x) for x in positiveFolder]\n",
    "negativePath = [os.path.join(negative_path, x) for x in negativeFolder]\n",
    "\n",
    "# Resize positive example and append with label as CAR\n",
    "for path in positivePath:\n",
    "    img = load_img(path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = preprocess_input(img)\n",
    "    data.append(img)\n",
    "    label = path.split(os.path.sep)[-2]\n",
    "    labels.append(label)\n",
    "\n",
    "# Resize negative example and append with label as NO_CAR\n",
    "for path in negativePath:\n",
    "    count += 1\n",
    "    if (count == 400): break # break at 400 negative examples (half of positive examples)\n",
    "    img = load_img(path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = preprocess_input(img)\n",
    "    data.append(img)\n",
    "    label = path.split(os.path.sep)[1]\n",
    "    labels.append(label)"
   ]
  },
  {
   "source": [
    "# Functions for re-use"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Retrieving bbox dimensions "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCar(annotations):\n",
    "    # Open annotation.json\n",
    "    f = open(annotations, 'r')\n",
    "    train = json.load(f)\n",
    "\n",
    "    f.close()\n",
    "    # Getting bbox coordinates\n",
    "    top = int(train[0][\"bbox\"][\"top\"])\n",
    "    bottom = int(train[0][\"bbox\"][\"bottom\"])\n",
    "    left = int(train[0][\"bbox\"][\"left\"])\n",
    "    right = int(train[0][\"bbox\"][\"right\"])\n",
    "\n",
    "    return left, top, right, bottom"
   ]
  },
  {
   "source": [
    "# Computes intersection of union ratio #\n",
    "## Adapted from Rosebrock's tutorial ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIoU(A, B):\n",
    "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
    "\tX1 = max(A[0], B[0]) # left\n",
    "\tY1 = max(A[1], B[1]) # top\n",
    "\tX2 = min(A[2], B[2]) # right\n",
    "\tY2 = min(A[3], B[3]) # bottom\n",
    "\n",
    "\t# compute the area of intersection rectangle\n",
    "\tinterArea = max(0, X2 - X1 + 1) * max(0, Y2 - Y1 + 1)\n",
    "\n",
    "\t# compute the area of both the prediction and ground-truth\n",
    "\tarea_A = (A[2] - A[0] + 1) * (A[3] - A[1] + 1)\n",
    "\tarea_B = (B[2] - B[0] + 1) * (B[3] - B[1] + 1)\n",
    "\t# compute the intersection over union by taking the intersection\n",
    "\t# area and dividing it by the sum of prediction + ground-truth\n",
    "\t# areas - the intersection area\n",
    "\tiou = interArea / float(area_A + area_B - interArea)\n",
    "\n",
    "\t# return the intersection over union value\n",
    "\treturn iou"
   ]
  },
  {
   "source": [
    "# Performs non-max suppression to remove overlapping bboxes #\n",
    "## Adapted from Rosebrock's tutorial ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(boxes, overlapThresh):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    pick = []\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "\n",
    "    while len(idxs) > 0:\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        suppress = [last]\n",
    "\n",
    "        for pos in range(0, last):\n",
    "            j = idxs[pos]\n",
    "            xx1 = max(x1[i], x1[j])\n",
    "            yy1 = max(y1[i], y1[j])\n",
    "            xx2 = min(x2[i], x2[j])\n",
    "            yy2 = min(y2[i], y2[j])\n",
    "            w = max(0, xx2 - xx1 + 1)\n",
    "            h = max(0, yy2 - yy1 + 1)\n",
    "            overlap = float(w * h) / area[j]\n",
    "            if overlap > overlapThresh:\n",
    "                suppress.append(pos)\n",
    "        idxs = np.delete(idxs, suppress)\n",
    "    return boxes[pick]"
   ]
  },
  {
   "source": [
    "# Flexible parameters for experimentation #"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01 # learning rate\n",
    "BS = 32 # batch size\n",
    "EPOCHS = 5 # num of epochs"
   ]
  },
  {
   "source": [
    "# CNN Architecture #\n",
    "## Adapted from Rosebrock's Tutorial ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "for layer in baseModel.layers:\n",
    "\tlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=lr)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "H = model.fit(X_train, y_train,\n",
    "\tsteps_per_epoch=len(X_train) // BS,\n",
    "\tvalidation_data=(X_test, y_test),\n",
    "\tvalidation_steps=len(X_test) // BS,\n",
    "\tepochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageDataset = np.array(data, dtype = \"float32\")\n",
    "labelSet = np.array(labels)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "labelSet = lb.fit_transform(labelSet)\n",
    "labelSet = to_categorical(labelSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imageDataset, labelSet, test_size = 0.33)"
   ]
  },
  {
   "source": [
    "# Evaluation of RCNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_image.png') # image to test on\n",
    "\n",
    "# Get proposals\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "ss.setBaseImage(img)\n",
    "ss.switchToSelectiveSearchFast()\n",
    "rects = ss.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the list of region proposals that we'll be classifying\n",
    "# along with their associated bounding boxes\n",
    "proposals = []\n",
    "boxes = []\n",
    "# loop over the region proposal bounding box coordinates generated by\n",
    "# running selective search\n",
    "for (x, y, w, h) in rects[:2000]:\n",
    "\troi = img[y:y + h, x:x + w]\n",
    "\troi = preprocess_input(roi)\n",
    "\t# update our proposals and bounding boxes lists\n",
    "\tproposals.append(roi)\n",
    "\tboxes.append((x, y, x + w, y + h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the proposals and bounding boxes into NumPy arrays\n",
    "proposals = np.array(proposals, dtype=\"float32\")\n",
    "boxes = np.array(boxes, dtype=\"int32\")\n",
    "# classify each of the proposal ROIs using fine-tuned model\n",
    "proba = model.predict(proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copies for quick re-use/experimenting\n",
    "cloneBoxes = boxes\n",
    "cloneProb = proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get result for each detection\n",
    "labels = np.argmax(cloneProb, axis = 1)\n",
    "# take results that are cars == 0\n",
    "found = np.where(labels == 0)[0]\n",
    "cloneBoxes = cloneBoxes[found]\n",
    "cloneProb = cloneProb[found][:, 1]\n",
    "# further filter indexes using a minimum probability\n",
    "found = np.where(1-cloneProb > 0.9999999)\n",
    "cloneBoxes = cloneBoxes[found]\n",
    "cloneProb = cloneProb[found]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'non_max_suppression' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bff5708080f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# perform non-max suppression to remove overlapping bboxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfinalBox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcloneBoxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'non_max_suppression' is not defined"
     ]
    }
   ],
   "source": [
    "# perform non-max suppression to remove overlapping bboxes\n",
    "finalBox = non_max_suppression(cloneBoxes, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_image.png')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# draw bboxes on detected cars\n",
    "for box in finalBox:\n",
    "    (startX, startY, endX, endY) = box\n",
    "    cv2.rectangle(img, (startX, startY), (endX, endY), (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ]
}